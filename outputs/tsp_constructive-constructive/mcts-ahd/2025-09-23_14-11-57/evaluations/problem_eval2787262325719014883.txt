def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    immediate_distances = []
    future_potentials = []
    exploration_penalties = []

    remaining_nodes = len(unvisited_nodes)
    total_nodes = len(distance_matrix)
    phase = 1 - (remaining_nodes / total_nodes)

    for node in unvisited_nodes:
        immediate_distance = distance_matrix[current_node][node]
        future_potential = distance_matrix[node][destination_node]

        immediate_distances.append(immediate_distance)
        future_potentials.append(future_potential)

        # Exploration penalty based on remaining nodes
        penalty = 0.2 * (1.0 / (immediate_distance + 1e-6)) * (1.0 + 0.3 * (1.0 - phase))
        exploration_penalties.append(penalty)

    # Adaptive weights based on phase
    immediate_weight = 0.5 * (1 - phase**2)
    future_weight = 0.3 * phase
    exploration_weight = 0.2 * phase**2

    # Normalize metrics
    max_immediate = max(immediate_distances) if immediate_distances else 1.0
    max_future = max(future_potentials) if future_potentials else 1.0
    max_penalty = max(exploration_penalties) if exploration_penalties else 1.0

    normalized_immediate = [d / max_immediate for d in immediate_distances]
    normalized_future = [p / max_future for p in future_potentials]
    normalized_penalty = [p / max_penalty for p in exploration_penalties]

    # Combine weighted scores
    scores = [immediate_weight * ni + future_weight * nf + exploration_weight * np
              for ni, nf, np in zip(normalized_immediate, normalized_future, normalized_penalty)]
    min_score = min(scores)
    selected_index = scores.index(min_score)
    next_node = list(unvisited_nodes)[selected_index]

    return next_node

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    remaining_nodes = len(unvisited_nodes)
    temperature = 1.0 / (1.0 + 0.2 * remaining_nodes)

    immediate_rewards = []
    future_rewards = []
    node_list = list(unvisited_nodes)

    for node in node_list:
        immediate_reward = -distance_matrix[current_node][node]

        if remaining_nodes > 2:
            next_nodes = [n for n in node_list if n != node]
            future_reward = -min(distance_matrix[node][n] + distance_matrix[n][destination_node] for n in next_nodes)
        else:
            future_reward = -distance_matrix[node][destination_node]

        immediate_rewards.append(immediate_reward)
        future_rewards.append(future_reward)

    max_immediate = max(immediate_rewards) if immediate_rewards else 0
    max_future = max(future_rewards) if future_rewards else 0

    normalized_immediate = [r / max_immediate if max_immediate != 0 else 0 for r in immediate_rewards]
    normalized_future = [r / max_future if max_future != 0 else 0 for r in future_rewards]

    alpha = 0.6 + 0.4 * (remaining_nodes / len(distance_matrix))
    combined_rewards = [alpha * i + (1 - alpha) * f for i, f in zip(normalized_immediate, normalized_future)]

    probabilities = [math.exp(r / temperature) for r in combined_rewards]
    sum_prob = sum(probabilities)
    probabilities = [p / sum_prob for p in probabilities]

    selected_index = np.random.choice(len(node_list), p=probabilities)
    return next_node

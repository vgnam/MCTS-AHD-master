importance (measured by betweenness centrality) and a reinforcement learning-inspired mechanism that updates node preferences based on historical selection frequency, while maintaining stochastic selection through a temperature parameter that adapts to both remaining nodes and historical performance.}

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    remaining_nodes = len(unvisited_nodes)
    temperature = max(0.1, 1.0 - (remaining_nodes / len(distance_matrix)) * 0.5)

    # Initialize or update node preferences (simplified RL mechanism)
    if not hasattr(select_next_node, 'node_preferences'):
        select_next_node.node_preferences = {node: 1.0 for node in range(len(distance_matrix))}
    else:
        for node in unvisited_nodes:
            if node in select_next_node.node_preferences:
                select_next_node.node_preferences[node] *= 0.95  # Decay over time

    scores = []
    for node in unvisited_nodes:
        immediate_distance = distance_matrix[current_node][node]
        future_potential = distance_matrix[node][destination_node]

        # Calculate dynamic centrality (betweenness centrality approximation)
        centrality = sum(1 / (distance_matrix[node][other] + 1e-6) for other in unvisited_nodes if other != node) / len(unvisited_nodes)

        # Dynamic weight adjustment based on node importance
        importance_weight = 0.5 if centrality > 0.7 else 0.3
        combined_score = (0.4 * immediate_distance +
                          0.3 * future_potential +
                          importance_weight * centrality)

        # Incorporate historical preferences
        combined_score *= select_next_node.node_preferences[node]

        # Apply temperature
        adjusted_score = combined_score * temperature
        scores.append(adjusted_score)

    # Update preferences based on selection probabilities
    min_score = min(scores)
    max_score = max(scores)
    normalized_scores = [(max_score - s) / (max_score - min_score) for s in scores]

    selected_index = normalized_scores.index(max(normalized_scores))
    next_node = list(unvisited_nodes)[selected_index]

    # Update node preferences based on selection
    for i, node in enumerate(unvisited_nodes):
        if i == selected_index:
            select_next_node.node_preferences[node] *= 1.2  # Reward selected node
        else:
            select_next_node.node_preferences[node] *= 0.9  # Penalize others

    return next_node

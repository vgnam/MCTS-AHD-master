importance metric combining degree centrality and distance centrality, uses a reinforcement learning-inspired exploration bonus with a decaying temperature parameter, and implements a memory-based revisit penalty with exponential growth to balance exploration and exploitation while preventing cycles.}

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(distance_matrix)
    remaining_nodes = len(unvisited_nodes)
    progress = 1 - remaining_nodes / total_nodes

    # Dynamic phase transitions
    phase = min(1, 2 * progress) if progress <= 0.5 else max(0, 2 * (progress - 0.5))

    # Node importance metric
    importance_scores = []
    for node in unvisited_nodes:
        degree_centrality = sum(1 for d in distance_matrix[node] if d > 0) / (total_nodes - 1)
        distance_centrality = sum(distance_matrix[node]) / (total_nodes - 1)
        importance_scores.append(degree_centrality * (1 / (1 + distance_centrality)))

    # Reinforcement learning exploration bonus
    temperature = 1 - (progress ** 2)
    exploration_bonus = []
    for i, node in enumerate(unvisited_nodes):
        immediate_distance = distance_matrix[current_node][node]
        normalized_importance = importance_scores[i] / max(1, max(importance_scores)) if importance_scores else 0
        exploration_bonus.append(normalized_importance * (1 / (1 + immediate_distance)) * temperature)

    # Memory-based revisit penalty
    revisit_penalty = []
    revisit_count = total_nodes - remaining_nodes - 1
    for node in unvisited_nodes:
        immediate_distance = distance_matrix[current_node][node]
        avg_distance = sum(sum(row) for row in distance_matrix) / (total_nodes * (total_nodes - 1))
        penalty = (revisit_count / total_nodes) ** 2 * (immediate_distance / avg_distance) if revisit_count > 0 else 0
        revisit_penalty.append(penalty)

    # Adaptive weighting
    distance_weight = 0.7 - 0.5 * phase
    future_weight = 0.3 + 0.5 * phase
    exploration_weight = 0.2 * (1 - phase)

    scores = []
    for i, node in enumerate(unvisited_nodes):
        immediate_distance = distance_matrix[current_node][node]
        future_distance = distance_matrix[node][destination_node]

        combined_score = (distance_weight * immediate_distance) + \
                         (future_weight * (1 / (1 + future_distance))) + \
                         revisit_penalty[i] - \
                         (exploration_weight * exploration_bonus[i])

        scores.append(combined_score)

    selected_index = scores.index(min(scores))
    next_node = list(unvisited_nodes)[selected_index]

    return next_node

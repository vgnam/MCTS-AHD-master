importance, historical path patterns, and a novel "path momentum" factor that rewards consistent direction while penalizing detours, combined with a reinforcement learning-inspired update mechanism for exploration bonuses.
}

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(distance_matrix)
    remaining_nodes = len(unvisited_nodes)
    progress = 1 - remaining_nodes / total_nodes

    # Dynamic phase transition system
    phase = min(1.0, 2 * progress) if progress <= 0.5 else max(0.0, 2 * (1 - progress))

    # Calculate node importance and path momentum
    node_importance = {}
    path_momentum = {}
    for node in unvisited_nodes:
        node_importance[node] = sum(1 / (1 + d) for d in distance_matrix[node]) / (total_nodes - 1)
        path_momentum[node] = distance_matrix[current_node][node] * (1 - phase)

    # Adaptive weight calculation
    immediate_weight = 0.7 * (1 - phase)
    future_weight = 0.3 * phase
    exploration_weight = 0.2 + 0.6 * (1 - phase)

    # Reinforcement learning-inspired exploration bonus
    exploration_bonus = {}
    for node in unvisited_nodes:
        centrality = node_importance[node]
        distance_factor = 1 / (1 + distance_matrix[current_node][node])
        exploration_bonus[node] = centrality * distance_factor * exploration_weight * (1 + 0.5 * (1 - phase))

    # Calculate combined scores
    scores = []
    for node in unvisited_nodes:
        immediate_cost = immediate_weight * distance_matrix[current_node][node]
        future_cost = future_weight * distance_matrix[node][destination_node]
        momentum_penalty = path_momentum[node] * (1 - phase)

        combined_score = immediate_cost + future_cost + momentum_penalty - exploration_bonus[node]
        scores.append(combined_score)

    selected_index = scores.index(min(scores))
    next_node = list(unvisited_nodes)[selected_index]

    return next_node

[
     {
          "algorithm": "The algorithm adaptively balances immediate distance, future potential (weighted dynamically by progress), and penalties for reconsidered nodes, prioritizing immediate distances early and future costs later while penalizing late reconsiderations. It uses a normalized progress metric (0 to 1) to adjust the dynamic weight, decreasing future cost influence as the tour progresses. The weighted score combines these factors, with penalties applied only after half the nodes are visited.",
          "thought": "The new algorithm combines No.1's weighted balance between immediate and future distances with No.2's dynamic weight adjustment and penalty mechanism. It adaptively shifts focus from immediate distances to future costs while penalizing reconsidered nodes late in the process, using a normalized progress metric to modulate the dynamic weight.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    best_node = None\n    min_weighted_score = float('inf')\n    progress = 1 - len(unvisited_nodes) / (len(unvisited_nodes) + 1)  # Normalized progress (0 to 1)\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_potential = distance_matrix[node][destination_node]\n\n        if len(unvisited_nodes) > 1:\n            remaining_nodes = [n for n in unvisited_nodes if n != node]\n            avg_remaining_dist = sum(distance_matrix[node][n] for n in remaining_nodes) / len(remaining_nodes)\n            penalty = sum(distance_matrix[current_node][n] for n in remaining_nodes) / len(remaining_nodes) if progress > 0.5 else 0\n        else:\n            avg_remaining_dist = 0\n            penalty = 0\n\n        dynamic_weight = 0.6 - 0.4 * progress  # Decrease future weight as progress increases\n        weighted_score = immediate_distance + dynamic_weight * future_potential + penalty\n\n        if weighted_score < min_weighted_score:\n            min_weighted_score = weighted_score\n            best_node = node\n\n    return next_node",
          "objective": 6.44101,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm prioritizes immediate distances (given a fixed weight of 0.5) while considering future potential (distance to the destination) for node selection. It applies a quadratic penalty to reconsidered nodes only after 70% progress, dynamically adjusting the penalty strength based on a linear progress metric. The code balances short-term and long-term considerations, with penalties only activated late in the process to refine choices.",
          "thought": "The new algorithm prioritizes immediate distances with a fixed future potential weight, applies a quadratic penalty for reconsidered nodes only after 70% progress, and uses a linear progress metric to dynamically adjust the penalty strength.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    best_node = None\n    min_weighted_score = float('inf')\n    progress = 1 - len(unvisited_nodes) / (len(unvisited_nodes) + 1)  # Linear progress metric (0 to 1)\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_potential = distance_matrix[node][destination_node]\n\n        if len(unvisited_nodes) > 1:\n            remaining_nodes = [n for n in unvisited_nodes if n != node]\n            avg_remaining_dist = sum(distance_matrix[node][n] for n in remaining_nodes) / len(remaining_nodes)\n            penalty = (sum(distance_matrix[current_node][n] for n in remaining_nodes) / len(remaining_nodes)) ** 2 if progress > 0.7 else 0\n        else:\n            avg_remaining_dist = 0\n            penalty = 0\n\n        dynamic_weight = 0.5  # Fixed future potential weight\n        weighted_score = immediate_distance + dynamic_weight * future_potential + penalty\n\n        if weighted_score < min_weighted_score:\n            min_weighted_score = weighted_score\n            best_node = node\n\n    return next_node",
          "objective": 6.44688,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm dynamically balances exploration and exploitation by adjusting weights for immediate distances (prioritized early) and future centrality (emphasized later), with a linear exploration bonus that fades as nodes are visited, resulting in a smooth transition. It avoids reconsideration penalties and uses a combined score of weighted immediate and future distances, with exploration bonuses, to select the next node. The progress variable (1 - remaining_nodes/total_nodes) controls the shift from distance-focused to centrality-focused selection.",
          "thought": "The new algorithm modifies the original by using a dynamic weight that emphasizes immediate distances early and future centrality later, with a linear exploration bonus and no reconsideration penalties, creating a smoother transition between exploration and exploitation phases.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress = 1 - remaining_nodes / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n\n        node_centrality = sum(distance_matrix[node]) / (total_nodes - 1)\n\n        distance_weight = 0.8 - 0.6 * progress\n        future_weight = 0.2 + 0.6 * progress\n\n        exploration_bonus = (remaining_nodes / total_nodes) * (1 + (node_centrality / max(distance_matrix[node])))\n\n        reconsideration_penalty = 0\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) - \\\n                         (0.2 * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.50876,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm dynamically selects the next node in TSP by balancing immediate distance, future potential, and node centrality, with weights adjusted based on progress (early stages prioritize exploration and centrality, later stages favor efficiency). It uses a progress-dependent centrality influence and exploration bonuses scaled by distance and remaining nodes, with the combined score minimizing immediate distance while considering future paths and node importance. The algorithm prioritizes exploration in early stages and efficiency in later stages, with centrality influencing both future potential and exploration bonuses.",
          "thought": "The new algorithm dynamically balances immediate distance, future potential, node centrality, and exploration diversity by adjusting weights based on progress (early stages prioritize exploration and centrality, later stages favor efficiency and future potential), using a progress-dependent centrality influence and exploration bonuses scaled by both distance and remaining nodes.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress_factor = (total_nodes - remaining_nodes) / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n\n        node_centrality = sum(distance_matrix[node]) / (total_nodes - 1)\n\n        distance_weight = (1 - progress_factor) * 0.6 + 0.4 - (0.15 * (node_centrality / max(distance_matrix[node])))\n        future_weight = progress_factor * 0.5 + 0.5 + (0.25 * (node_centrality / max(distance_matrix[node])))\n        exploration_weight = (1 - progress_factor) * 0.4 + 0.6\n\n        exploration_bonus = (1 / (1 + immediate_distance)) * (remaining_nodes / total_nodes) * (1 + (2 * (node_centrality / max(distance_matrix[node]))))\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) - \\\n                         (exploration_weight * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.52656,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm combines a greedy approach with dynamic weighting and multi-step lookahead to balance immediate and future distances. It prioritizes immediate distances early in the search (higher `immediate_weight`) and future potential later (higher `future_weight`), while applying a memory penalty to discourage revisiting nodes. The `exploration_factor` encourages exploration in early stages, and normalization ensures fair comparison across nodes. The critical design choices are the dynamic adjustment of weights based on remaining nodes and the memory penalty to avoid cycles.",
          "thought": "This new algorithm combines the greedy lookahead mechanism of No.1 with the dynamic weighting and exploration penalty of No.2, while introducing a memory-based penalty to avoid revisiting recently visited nodes and a multi-step lookahead to better balance immediate and future distances.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    remaining_nodes = len(unvisited_nodes)\n    base_immediate_weight = 0.5\n    base_future_weight = 0.5\n    stage_factor = 1.0 / (1.0 + 0.1 * remaining_nodes)\n\n    immediate_weight = base_immediate_weight * (1.0 - stage_factor)\n    future_weight = base_future_weight * (1.0 + stage_factor)\n\n    exploration_factor = 1.0 + 0.4 * (1.0 - (remaining_nodes / len(distance_matrix)))\n\n    best_node = None\n    best_score = float('inf')\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n\n        if remaining_nodes > 2:\n            next_nodes = [n for n in unvisited_nodes if n != node]\n            future_potential = min(distance_matrix[node][n] + distance_matrix[n][destination_node] for n in next_nodes)\n        else:\n            future_potential = distance_matrix[node][destination_node]\n\n        normalized_immediate = immediate_distance / (sum(distance_matrix[current_node]) / len(distance_matrix))\n        normalized_future = future_potential / (sum(distance_matrix[node]) / len(distance_matrix))\n\n        memory_penalty = 0.2 * (1.0 / (immediate_distance + 1e-6)) * exploration_factor\n        combined_score = immediate_weight * normalized_immediate + future_weight * normalized_future - memory_penalty\n\n        if combined_score < best_score:\n            best_score = combined_score\n            best_node = node\n\n    return next_node",
          "objective": 6.52839,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm dynamically balances exploration and exploitation by prioritizing immediate distances early in the search (using a phase-based weighting system) while gradually favoring future costs (to the destination) and penalizing revisits to avoid cycles. It incorporates probabilistic exploration bonuses based on node centrality and distance, with higher weights given to central nodes early on, and a memory-based penalty that increases with revisits to encourage diverse paths. The selection is made by minimizing a combined score that adaptively adjusts weights between immediate, future, and exploration factors.",
          "thought": "This new algorithm introduces a dynamic phase-based weighting system that adapts to the search phase (early vs. late) by using non-linear progress weighting, incorporates a probabilistic exploration component based on node centrality and distance, and applies a memory-based penalty mechanism that increases with revisits to encourage diverse paths, while maintaining a balance between immediate and future costs through adaptive weight adjustments.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress = 1 - remaining_nodes / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n\n        node_centrality = sum(distance_matrix[node]) / (total_nodes - 1)\n        avg_distance = sum(sum(row) for row in distance_matrix) / (total_nodes * (total_nodes - 1))\n\n        phase_factor = (progress ** 2) if progress <= 0.5 else (1 - (1 - progress) ** 2)\n        distance_weight = 0.8 - 0.6 * phase_factor\n        future_weight = 0.2 + 0.6 * phase_factor\n\n        exploration_prob = (node_centrality / avg_distance) * (1 - progress)\n        exploration_bonus = (1 / (1 + immediate_distance)) * exploration_prob * (1 + (2 * (node_centrality / max(distance_matrix[node]))))\n\n        revisit_penalty = 0\n        if progress > 0.5:\n            revisit_count = total_nodes - remaining_nodes - 1\n            revisit_penalty = (revisit_count / total_nodes) * (immediate_distance / avg_distance)\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) + \\\n                         revisit_penalty - \\\n                         (0.4 * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.54022,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm combines No.1's dynamic weight adjustment and exploration bonus with No.2's centrality-based future weighting, prioritizing immediate distance early (high `distance_weight`) and future potential (low `future_distance`) later (high `future_weight`), while balancing exploration via an `exploration_bonus` that depends on node centrality and remaining nodes. The `combined_score` minimizes immediate distance while maximizing future potential and exploration, with weights modulated by progress (`progress_factor`). Higher priority is given to immediate distance early, while future potential and exploration gain importance as progress increases.",
          "thought": "The new algorithm combines No.1's dynamic weight adjustment and exploration bonus with No.2's centrality-based future weighting, creating a hybrid that balances immediate distance, future potential, and node diversity more effectively by using a progress-dependent weight modulation and a refined exploration bonus that considers both distance and centrality.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress_factor = (total_nodes - remaining_nodes) / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n        node_centrality = sum(distance_matrix[node]) / (total_nodes - 1)\n\n        distance_weight = (1 - progress_factor) * 0.7 + 0.3\n        future_weight = progress_factor * 0.6 + 0.4\n\n        exploration_bonus = (remaining_nodes / total_nodes) * (1 / (1 + immediate_distance)) * (1 + (node_centrality / max(distance_matrix[node])))\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) - \\\n                         (0.3 * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.54172,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm prioritizes immediate distance (highest weight) and future connectivity diversity (high weight) early, gradually shifting to regret (medium weight) and local density (lowest weight) as nodes are visited, using linear decay of weights to balance criteria throughout the selection process. The code calculates multiple metrics (distance, regret, diversity, density) for each unvisited node, combines them with dynamically adjusted weights, and selects the node with the minimum weighted score.",
          "thought": "The new algorithm prioritizes immediate distance and future connectivity diversity early with high weights, gradually shifting to local density and regret as nodes are visited, using linear decay of weights to maintain balance throughout the selection process.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    distances = []\n    regrets = []\n    future_diversities = []\n    local_densities = []\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_potential = distance_matrix[node][destination_node]\n        distances.append(immediate_distance)\n\n        other_nodes = [n for n in unvisited_nodes if n != node]\n        if other_nodes:\n            other_distances = [distance_matrix[current_node][n] for n in other_nodes]\n            second_best = min(other_distances)\n            regret = immediate_distance - second_best\n        else:\n            regret = 0\n        regrets.append(regret)\n\n        future_diversity = sum(1 for n in unvisited_nodes if n != node and distance_matrix[node][n] < 1.5 * immediate_distance)\n        future_diversities.append(future_diversity)\n\n        nearby_distances = [distance_matrix[node][n] for n in unvisited_nodes if n != node]\n        local_density = 1 / (sum(nearby_distances) / len(nearby_distances)) if nearby_distances else 0\n        local_densities.append(local_density)\n\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    distance_weight = 0.6 - (0.4 * remaining_nodes / total_nodes)\n    diversity_weight = 0.4 - (0.2 * remaining_nodes / total_nodes)\n    regret_weight = 0.2 * (1 - remaining_nodes / total_nodes)\n    density_weight = 0.1 * (1 - remaining_nodes / total_nodes)\n\n    scores = [\n        distance_weight * distance + diversity_weight * diversity +\n        regret_weight * regret + density_weight * density\n        for distance, diversity, regret, density in zip(\n            distances, future_diversities, regrets, local_densities\n        )\n    ]\n    min_score = min(scores)\n    selected_index = scores.index(min_score)\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.56813,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm dynamically balances immediate distance, future potential, and exploration diversity by adjusting weights based on progress (earlier stages prioritize exploration, later stages favor efficiency). It uses a progress factor to modulate weights (distance_weight decreases, future_weight increases) and incorporates an exploration bonus scaled inversely with distance and remaining nodes. The combined score prioritizes minimizing immediate and future distances while encouraging diversity early on.",
          "thought": "The new algorithm modifies the original by incorporating a dynamic weight adjustment mechanism that balances immediate distance, future potential, and a novel \"exploration bonus\" term to encourage diversity in early selection while maintaining efficiency as the path nears completion.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress_factor = (total_nodes - remaining_nodes) / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n\n        # Dynamic weight adjustment\n        distance_weight = (1 - progress_factor) * 0.8 + 0.2\n        future_weight = progress_factor * 0.7 + 0.3\n\n        # Exploration bonus for diversity\n        exploration_bonus = (1 / (1 + immediate_distance)) * (remaining_nodes / total_nodes)\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) - \\\n                         (0.5 * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.57163,
          "other_inf": null
     },
     {
          "algorithm": "The algorithm dynamically balances immediate distance (prioritized early) and future centrality (prioritized later) using a quadratic exploration bonus that decays with progress. It avoids reconsideration penalties and selects the next node based on a weighted score of distances and exploration bonuses, with weights adjusting non-linearly based on remaining nodes. The code uses `distance_weight` (decreasing early) and `future_weight` (increasing late) to guide selection, while `exploration_bonus` encourages visiting central nodes later.",
          "thought": "The new algorithm dynamically adjusts weights for immediate distances (emphasized early) and future centrality (emphasized later) with a quadratic exploration bonus that decays based on remaining nodes, creating a non-linear transition, while avoiding reconsideration penalties and using a combined score of weighted distances and exploration bonuses to select the next node.",
          "code": "def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    if not unvisited_nodes:\n        return destination_node\n\n    scores = []\n    total_nodes = len(distance_matrix)\n    remaining_nodes = len(unvisited_nodes)\n    progress = 1 - remaining_nodes / total_nodes\n\n    for node in unvisited_nodes:\n        immediate_distance = distance_matrix[current_node][node]\n        future_distance = distance_matrix[node][destination_node]\n\n        node_centrality = sum(distance_matrix[node]) / (total_nodes - 1)\n\n        distance_weight = 0.7 - 0.5 * (progress ** 2)\n        future_weight = 0.3 + 0.5 * (progress ** 2)\n\n        exploration_bonus = (remaining_nodes / total_nodes) ** 2 * (1 + (node_centrality / max(distance_matrix[node])))\n\n        reconsideration_penalty = 0\n\n        combined_score = (distance_weight * immediate_distance) + \\\n                         (future_weight * (1 / (1 + future_distance))) - \\\n                         (0.3 * exploration_bonus)\n\n        scores.append(combined_score)\n\n    selected_index = scores.index(min(scores))\n    next_node = list(unvisited_nodes)[selected_index]\n\n    return next_node",
          "objective": 6.57488,
          "other_inf": null
     }
]
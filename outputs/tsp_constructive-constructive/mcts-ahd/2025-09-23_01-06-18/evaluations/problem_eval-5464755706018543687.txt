def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node
    if destination_node in unvisited_nodes:
        remaining_distance = distance_matrix[current_node][destination_node]
        avg_distance = sum(distance_matrix[current_node][node] for node in unvisited_nodes) / len(unvisited_nodes)
        if avg_distance == 0:
            return destination_node
        threshold = 1.0 + 0.2 * (1.0 - (len(unvisited_nodes) / len(distance_matrix)))
        if remaining_distance / avg_distance <= threshold:
            return destination_node

    # Calculate betweenness centrality for each node as a connectivity measure
    betweenness = {}
    for node in unvisited_nodes:
        if node == destination_node:
            continue
        # Simple betweenness approximation: count how many shortest paths pass through this node
        betweenness[node] = sum(
            1 for other in unvisited_nodes
            if other != node and distance_matrix[current_node][node] + distance_matrix[node][destination_node] < 1.2 * distance_matrix[current_node][destination_node]
        )

    # Dynamic temperature for probabilistic selection
    temperature = 1.0 + 0.5 * (len(unvisited_nodes) / len(distance_matrix))

    # Reinforcement learning inspired reward system
    rewards = {}
    for node in unvisited_nodes:
        to_current = distance_matrix[current_node][node]
        to_destination = distance_matrix[node][destination_node]
        remaining_ratio = len(unvisited_nodes) / len(distance_matrix)

        # Base reward combines distance and connectivity
        base_reward = (1 - remaining_ratio) * to_current + remaining_ratio * to_destination

        # Connectivity bonus
        connectivity_bonus = 0.2 * betweenness.get(node, 0) / len(unvisited_nodes)

        # Exploration bonus
        exploration_bonus = 0.1 * (1 / (1 + to_current)) * (1 - remaining_ratio)

        rewards[node] = base_reward - connectivity_bonus + exploration_bonus

    # Temperature-based probabilistic selection
    exp_rewards = {node: math.exp(reward / temperature) for node, reward in rewards.items()}
    total = sum(exp_rewards.values())
    probs = {node: reward / total for node, reward in exp_rewards.items()}

    # Select node with highest probability
    next_node = max(probs.items(), key=lambda x: x[1])[0]
    return next_node

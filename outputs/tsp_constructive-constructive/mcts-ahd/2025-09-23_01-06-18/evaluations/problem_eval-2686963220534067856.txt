def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node
    if destination_node in unvisited_nodes:
        remaining_distance = distance_matrix[current_node][destination_node]
        avg_distance = sum(distance_matrix[current_node][node] for node in unvisited_nodes) / len(unvisited_nodes)
        if avg_distance == 0:
            return destination_node
        threshold = 1.0 + 0.3 * (1.0 - (len(unvisited_nodes) / len(distance_matrix)))
        if remaining_distance / avg_distance <= threshold:
            return destination_node

    def heuristic(node):
        to_current = distance_matrix[current_node][node]
        to_destination = distance_matrix[node][destination_node]
        remaining_ratio = len(unvisited_nodes) / len(distance_matrix)

        # Dynamic priority system
        local_priority = to_current * (1.0 + 0.5 * remaining_ratio)
        global_priority = to_destination * (1.0 - 0.5 * remaining_ratio)

        # Anticipation term: predict future costs
        anticipation = sum(sorted(distance_matrix[node][other] for other in unvisited_nodes)[:2]) / 2

        # Hubness score: measure node connectivity
        hubness = sum(1.0 / (1.0 + distance_matrix[node][other]) for other in unvisited_nodes) / len(unvisited_nodes)

        # Reinforcement learning-inspired exploration term
        exploration = 0.3 * len(unvisited_nodes) * (1.0 - hubness)

        # Dynamic weighting
        weight = 0.7 * (1.0 - remaining_ratio) + 0.3 * hubness

        return (local_priority + global_priority + anticipation) * (1.0 - weight) + exploration

    next_node = min(unvisited_nodes, key=heuristic)
    return next_node

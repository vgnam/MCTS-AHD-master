def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node
    if destination_node in unvisited_nodes:
        return destination_node

    remaining_nodes = len(unvisited_nodes)
    total_nodes = len(distance_matrix)
    progress = (total_nodes - remaining_nodes) / total_nodes

    # Calculate current tour quality (average distance)
    current_tour_quality = sum(distance_matrix[current_node][n] for n in unvisited_nodes) / remaining_nodes if remaining_nodes else 0

    # Dynamic regret scaling factor
    regret_scale = (1 - progress) * (1 + current_tour_quality / max(max(row) for row in distance_matrix))

    def heuristic(node):
        local_cost = distance_matrix[current_node][node]
        global_potential = distance_matrix[node][destination_node]
        regret = (local_cost - min(distance_matrix[current_node][n] for n in unvisited_nodes)) / (max(distance_matrix[current_node][n] for n in unvisited_nodes) - min(distance_matrix[current_node][n] for n in unvisited_nodes) + 1e-6)
        centrality = sum(distance_matrix[node][n] for n in unvisited_nodes) / (remaining_nodes + 1e-6)
        diversity_bonus = sum(distance_matrix[node][n] for n in unvisited_nodes) / (remaining_nodes + 1e-6)

        # Exploration incentive term
        exploration_incentive = (1 - progress) * (1 - regret) * sum(distance_matrix[node][n] for n in unvisited_nodes) / (remaining_nodes + 1e-6)

        return (regret_scale * regret) + (0.6 * local_cost + 0.4 * global_potential) - (0.3 * centrality) + (0.2 * exploration_incentive)

    next_node = min(unvisited_nodes, key=heuristic)
    return next_node

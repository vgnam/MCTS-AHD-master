def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node
    if destination_node in unvisited_nodes:
        remaining_distance = distance_matrix[current_node][destination_node]
        avg_distance = sum(distance_matrix[current_node][node] for node in unvisited_nodes) / len(unvisited_nodes)
        if avg_distance == 0:
            return destination_node
        threshold = 0.8 - (0.3 * (len(unvisited_nodes) / len(distance_matrix)))
        if remaining_distance / avg_distance <= threshold:
            return destination_node

    def heuristic(node):
        to_current = distance_matrix[current_node][node]
        to_destination = distance_matrix[node][destination_node]
        remaining_ratio = len(unvisited_nodes) / len(distance_matrix)
        weight = 1.8 - remaining_ratio

        # Reinforcement learning inspired weight adjustment
        if len(unvisited_nodes) > len(distance_matrix) // 2:
            weight = 1.5 - 0.5 * remaining_ratio
        else:
            weight = 1.2 + 0.3 * remaining_ratio

        # Path diversity term considering both immediate and long-term connectivity
        diversity_bonus = sum(distance_matrix[node][other] for other in unvisited_nodes) / (len(unvisited_nodes) + 1e-6)
        diversity_bonus += 0.3 * sum(distance_matrix[other][node] for other in unvisited_nodes) / (len(unvisited_nodes) + 1e-6)

        # Regret term with adaptive lookahead
        lookahead = min(3, len(unvisited_nodes))
        regret_term = sum(sorted(distance_matrix[current_node][other] - to_current for other in unvisited_nodes)[:lookahead]) / lookahead

        return (to_current * weight + to_destination * (1.0 - weight) + 0.7 * regret_term) - 0.3 * diversity_bonus

    next_node = min(unvisited_nodes, key=heuristic)
    return next_node

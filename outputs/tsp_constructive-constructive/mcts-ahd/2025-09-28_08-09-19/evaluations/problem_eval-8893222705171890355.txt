def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    next_node = None
    best_score = float('-inf')
    remaining_nodes = len(unvisited_nodes)
    total_nodes = len(distance_matrix)

    # Dynamic priority weights
    priority_weights = {node: 1.0 for node in unvisited_nodes}
    exploration_factor = 0.5 * (remaining_nodes / total_nodes)

    for node in unvisited_nodes:
        immediate_distance = distance_matrix[current_node][node]
        future_potential = sum(distance_matrix[node][n] for n in unvisited_nodes if n != node) / (remaining_nodes - 1) if remaining_nodes > 1 else 0
        destination_distance = distance_matrix[node][destination_node]

        # Reinforcement learning-inspired priority update
        priority_weights[node] *= (1 - exploration_factor) * (1.0 / (1.0 + immediate_distance)) + exploration_factor * (1.0 / (1.0 + destination_distance))

        # Dynamic exploration-exploitation balance
        exploitation_score = priority_weights[node] * (1.0 / (1.0 + immediate_distance))
        exploration_score = (1.0 / (1.0 + future_potential)) * (remaining_nodes / total_nodes)

        # Combined score
        score = (0.7 * exploitation_score) + (0.3 * exploration_score)

        if score > best_score:
            best_score = score
            next_node = node

    return next_node

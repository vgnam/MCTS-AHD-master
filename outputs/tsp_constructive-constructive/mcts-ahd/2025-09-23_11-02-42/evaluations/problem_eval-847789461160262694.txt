def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    next_node = None
    best_score = float('-inf')
    remaining_nodes = len(unvisited_nodes)

    # Precompute diversity and historical factors
    diversity_scores = []
    for node in unvisited_nodes:
        diversity = sum(abs(distance_matrix[node][other] - distance_matrix[current_node][other]) for other in unvisited_nodes if other != node)
        diversity_scores.append(diversity)

    avg_diversity = sum(diversity_scores) / len(diversity_scores) if diversity_scores else 0

    for i, node in enumerate(unvisited_nodes):
        distance_to_node = distance_matrix[current_node][node]
        distance_to_destination = distance_matrix[node][destination_node]

        # Adaptive weighting based on diversity
        diversity_factor = 1.0 + (diversity_scores[i] / (avg_diversity + 1e-6)) * 0.5

        if remaining_nodes == 1:
            score = - (distance_to_node + distance_to_destination) * diversity_factor
        else:
            direct_distance = distance_matrix[current_node][destination_node]
            detour_ratio = direct_distance / (distance_to_node + distance_to_destination)
            detour_penalty = (1.0 - detour_ratio) * direct_distance

            # Historical performance factor (simplified)
            historical_factor = 1.0 / (1.0 + detour_penalty)

            # Reinforcement learning-inspired exploration
            exploration_incentive = -0.2 * sum(distance_matrix[node][other] for other in unvisited_nodes if other != node) / (remaining_nodes - 1)

            score = (distance_to_node * 1.2 + 0.3 * detour_penalty * diversity_factor) * historical_factor + exploration_incentive

        if score > best_score:
            best_score = score
            next_node = node

    return next_node

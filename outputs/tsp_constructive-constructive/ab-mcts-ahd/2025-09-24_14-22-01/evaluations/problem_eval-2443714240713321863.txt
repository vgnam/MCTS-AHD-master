def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    min_cost = float('inf')
    next_node = None
    remaining_nodes = len(unvisited_nodes)
    avg_distance = sum(distance_matrix[current_node]) / len(distance_matrix[current_node])

    # Dynamic weight factor based on remaining nodes and historical performance
    weight_factor = 0.3 * (remaining_nodes / (remaining_nodes + 1)) + 0.7 * (1 - (remaining_nodes / len(distance_matrix)))

    # Novelty term to encourage exploration of less frequently visited nodes
    visit_frequency = {node: 1 / (1 + sum(row[node] for row in distance_matrix)) for node in unvisited_nodes}
    novelty = {node: visit_frequency[node] / sum(visit_frequency.values()) for node in unvisited_nodes}

    for node in unvisited_nodes:
        if node == destination_node:
            continue
        immediate_distance = distance_matrix[current_node][node]
        potential_distance = distance_matrix[node][destination_node]

        # Reinforcement learning-inspired penalty adjustment
        penalty = 0.15 * (immediate_distance / avg_distance) + 0.85 * (1 - novelty[node])

        weighted_cost = immediate_distance + weight_factor * potential_distance - penalty
        if weighted_cost < min_cost:
            min_cost = weighted_cost
            next_node = node

    if next_node is None:
        next_node = destination_node
    return next_node

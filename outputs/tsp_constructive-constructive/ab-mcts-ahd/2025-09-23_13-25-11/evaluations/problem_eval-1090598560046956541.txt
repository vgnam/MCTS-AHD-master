def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(unvisited_nodes) + 1  # +1 for current_node
    remaining_ratio = len(unvisited_nodes) / total_nodes if total_nodes > 0 else 0
    dynamic_weight = 0.3 + 0.7 * (1 - remaining_ratio)  # Higher weight for future as nodes decrease

    def evaluate_node(node):
        current_dist = distance_matrix[current_node][node]
        remaining_nodes = unvisited_nodes - {node}

        if not remaining_nodes:
            return current_dist + 0.8 * distance_matrix[node][destination_node]

        # Sample a subset of remaining nodes for probabilistic estimation
        sample_size = min(3, len(remaining_nodes))
        sampled_nodes = set(np.random.choice(list(remaining_nodes), sample_size, replace=False))
        avg_future_dist = sum(distance_matrix[node][other] for other in sampled_nodes) / sample_size

        # Adaptive penalty based on distance to destination
        dest_penalty = 0.2 * (distance_matrix[node][destination_node] - current_dist)
        dest_penalty = max(0, dest_penalty)  # Avoid negative penalties

        # Reinforcement learning-inspired term: prioritize nodes that reduce future variance
        variance_term = 0.1 * np.var([distance_matrix[node][other] for other in remaining_nodes])

        return (1.0 * current_dist +
                dynamic_weight * avg_future_dist +
                0.3 * dest_penalty -
                0.2 * variance_term)

    next_node = min(unvisited_nodes, key=evaluate_node)
    return next_node

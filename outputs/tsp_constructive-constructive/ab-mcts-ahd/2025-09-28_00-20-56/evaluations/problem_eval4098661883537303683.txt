def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    # Calculate initial metrics
    total_remaining_distance = sum(distance_matrix[node][destination_node] for node in unvisited_nodes)
    avg_remaining_distance = total_remaining_distance / len(unvisited_nodes)

    # Reinforcement learning-inspired node weights
    node_weights = {}
    for node in unvisited_nodes:
        current_dist = distance_matrix[current_node][node]
        if len(unvisited_nodes) > 1:
            remaining_nodes = [n for n in unvisited_nodes if n != node]
            best_lookahead = min(remaining_nodes, key=lambda n: distance_matrix[node][n] + distance_matrix[n][destination_node])
            lookahead_dist = distance_matrix[node][best_lookahead] + distance_matrix[best_lookahead][destination_node]

            # Novel centrality metric: distance + connectivity
            centrality = (sum(distance_matrix[node][n] for n in remaining_nodes) / len(remaining_nodes)) * (1 + 1/len(remaining_nodes))

            # Adaptive weight based on performance history (simplified)
            performance_factor = 1.0  # In practice, this would track historical success
            adjusted_lookahead_dist = lookahead_dist * (centrality ** 0.3) * performance_factor

            # Enhanced probabilistic component
            random_factor = 1.0 + 0.3 * (1 - (current_dist / (current_dist + lookahead_dist))) ** 2
        else:
            adjusted_lookahead_dist = distance_matrix[node][destination_node]
            random_factor = 1.0

        node_weights[node] = (current_dist + 0.6 * adjusted_lookahead_dist) * random_factor

    # Select node with highest weight (inverse of original's min approach)
    next_node = max(unvisited_nodes, key=lambda n: node_weights[n])
    return next_node

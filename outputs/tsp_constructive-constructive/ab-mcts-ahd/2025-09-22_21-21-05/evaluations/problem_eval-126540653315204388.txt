def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    best_score = float('-inf')
    next_node = None
    exploration_factor = 0.3  # Decays over iterations
    reward_decay = 0.9  # Discounts older rewards

    # Initialize or update historical rewards (simplified for demonstration)
    if not hasattr(select_next_node, 'historical_rewards'):
        select_next_node.historical_rewards = {node: 0 for node in distance_matrix}

    for node in unvisited_nodes:
        distance_to_current = distance_matrix[current_node][node]
        distance_to_destination = distance_matrix[node][destination_node]
        avg_distance_to_remaining = sum(distance_matrix[node][n] for n in unvisited_nodes if n != node) / len(unvisited_nodes) if unvisited_nodes else 0

        # Dynamic reward calculation
        reward = (distance_to_current + distance_to_destination) - avg_distance_to_remaining
        historical_reward = select_next_node.historical_rewards[node] * reward_decay
        select_next_node.historical_rewards[node] = historical_reward + reward

        # Exploration-adjusted score
        score = reward + exploration_factor * (1 / (1 + historical_reward))

        if score > best_score:
            best_score = score
            next_node = node

    return next_node

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    best_score = -float('inf')
    next_node = None
    num_unvisited = len(unvisited_nodes)
    exploration_factor = 0.7 ** (1 / (1 + num_unvisited))  # Decaying exploration factor

    for node in unvisited_nodes:
        # Immediate distance considerations
        distance_to_current = distance_matrix[current_node][node]
        distance_to_destination = distance_matrix[node][destination_node]
        sum_distances = distance_to_current + distance_to_destination

        # Global considerations
        distances_to_remaining = [distance_matrix[node][n] for n in unvisited_nodes if n != node]
        avg_distance_to_remaining = sum(distances_to_remaining) / len(unvisited_nodes) if unvisited_nodes else 0
        variance = sum((d - avg_distance_to_remaining) ** 2 for d in distances_to_remaining) / len(unvisited_nodes) if unvisited_nodes else 0

        # Dynamic weight adjustment
        dynamic_weight = exploration_factor * (1 / (1 + num_unvisited)) + (1 - exploration_factor) * (1 - (1 / (1 + num_unvisited)))

        # Reinforcement learning-inspired score
        combined_score = (sum_distances * dynamic_weight) + (avg_distance_to_remaining * (1 - dynamic_weight)) - (variance * 0.2) + (exploration_factor * 0.3 * sum_distances)

        if combined_score > best_score:
            best_score = combined_score
            next_node = node

    return next_node

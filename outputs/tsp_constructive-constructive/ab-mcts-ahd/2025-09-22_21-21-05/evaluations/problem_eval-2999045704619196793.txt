def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(distance_matrix)
    remaining_nodes = len(unvisited_nodes)
    progress_ratio = remaining_nodes / total_nodes

    # Dynamic weight adjustment based on progress and node properties
    proximity_weight = 1 - (progress_ratio ** 1.5)  # More emphasis on proximity early
    destination_weight = progress_ratio ** 1.5  # More emphasis on destination later

    best_score = float('-inf')
    next_node = None
    scores = []

    for node in unvisited_nodes:
        distance_to_current = distance_matrix[current_node][node]
        distance_to_destination = distance_matrix[node][destination_node]

        # Multi-criteria score with dynamic weights
        score = (proximity_weight * distance_to_current) - (destination_weight * distance_to_destination)

        # Penalty for nodes too close to destination early
        if progress_ratio > 0.5:  # Early phase
            avg_destination_distance = sum(distance_matrix[destination_node]) / total_nodes
            if distance_to_destination < 0.4 * avg_destination_distance:
                score *= 0.7  # Reduce score by 30% if too close

        scores.append((node, score))

    # Reinforcement learning-inspired selection
    if scores:
        # Sort by score (ascending for distance minimization)
        scores.sort(key=lambda x: x[1])

        # Select top candidate with probabilistic exploration
        top_candidates = [s[0] for s in scores[:3]]  # Consider top 3 candidates
        if len(top_candidates) > 1 and random.random() < 0.3:  # 30% chance to explore
            next_node = random.choice(top_candidates)
        else:
            next_node = scores[0][0]  # Best score

    return next_node

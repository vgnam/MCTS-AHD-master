def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(unvisited_nodes) + 1
    visited_ratio = (total_nodes - len(unvisited_nodes)) / total_nodes
    exploration_factor = len(unvisited_nodes) / total_nodes  # Encourages exploration

    # Dynamic local-global balance using sigmoid
    def sigmoid(x):
        return 1 / (1 + math.exp(-x))

    local_weight = sigmoid(visited_ratio - 0.5)  # Shifted to favor global later
    global_weight = 1 - local_weight

    def calculate_score(node):
        if not unvisited_nodes:
            return distance_matrix[current_node][node]

        distances_to_unvisited = [distance_matrix[node][n] for n in unvisited_nodes]
        avg_distance = sum(distances_to_unvisited) / len(unvisited_nodes)
        variance = sum((d - avg_distance) ** 2 for d in distances_to_unvisited) / len(unvisited_nodes)

        # Penalty based on distance to destination
        distance_to_dest = distance_matrix[node][destination_node]
        penalty = 0.3 * distance_to_dest  # Stronger penalty for distant nodes

        # Reinforcement learning-inspired exploration term
        exploration_term = exploration_factor * math.log(1 + len(unvisited_nodes))

        return (local_weight * distance_matrix[current_node][node] +
                global_weight * avg_distance +
                penalty -
                exploration_term)

    next_node = min(unvisited_nodes, key=calculate_score)
    return next_node

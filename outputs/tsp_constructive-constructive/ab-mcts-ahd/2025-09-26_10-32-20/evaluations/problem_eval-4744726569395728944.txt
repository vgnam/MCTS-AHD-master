def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(distance_matrix)
    remaining_ratio = len(unvisited_nodes) / total_nodes
    local_weight = 0.5 + 0.5 * (1 - remaining_ratio)  # Dynamic local weight
    global_weight = 1 - local_weight

    # Memory-based penalty for nodes with high historical costs
    historical_penalty = {}
    if hasattr(select_next_node, 'history'):
        for node in unvisited_nodes:
            historical_penalty[node] = sum(select_next_node.history.get((current_node, node), 0) for _ in range(3)) / 3
    else:
        historical_penalty = {node: 0 for node in unvisited_nodes}

    # Adaptive centrality with memory influence
    def centrality(node):
        base_centrality = 1 / (sum(distance_matrix[node][n] for n in range(total_nodes)) + 1e-10)
        memory_factor = 1 - (sum(historical_penalty.values()) / (len(unvisited_nodes) + 1e-10)) if historical_penalty else 0.5
        return base_centrality * (0.6 + 0.4 * memory_factor)

    # Hybrid score with memory-aware weighting
    hybrid_score = lambda node: (
        local_weight * distance_matrix[current_node][node] +
        global_weight * (0.6 * distance_matrix[node][destination_node] + 0.4 * centrality(node)) +
        0.2 * historical_penalty[node]  # Penalty term
    )

    next_node = min(unvisited_nodes, key=hybrid_score)

    # Update history (simplified for demonstration)
    if not hasattr(select_next_node, 'history'):
        select_next_node.history = {}
    select_next_node.history[(current_node, next_node)] = select_next_node.history.get((current_node, next_node), 0) + 1

    return next_node

{
     "algorithm": "This algorithm dynamically balances exploration and exploitation using a sigmoid-weighted scoring system that prioritizes proximity (exploitation) and connectivity (exploration) while penalizing revisited nodes and rewarding novel paths. It adjusts weights based on remaining unvisited nodes and incorporates historical path data for diversity, with local distance, connectivity, flow disruption, destination proximity, centrality, and novelty all contributing to the selection of the next node.",
     "thought": "The new algorithm modifies the scoring system to incorporate a dynamic exploration-exploitation balance using a sigmoid function to adjust weights, adds a penalty for revisiting nodes based on historical path data, and incorporates a novelty bonus for less-visited nodes to encourage diverse exploration while maintaining proximity and connectivity priorities.",
     "code": "import math\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix, historical_path=None):\n    if not unvisited_nodes:\n        return destination_node\n\n    remaining_nodes = len(unvisited_nodes)\n    total_nodes = len(distance_matrix)\n    exploration_factor = remaining_nodes / total_nodes\n\n    # Sigmoid function for dynamic weight adjustment\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n\n    exploration_weight = sigmoid(5 * (exploration_factor - 0.5))\n    exploitation_weight = 1 - exploration_weight\n\n    scores = []\n    for node in unvisited_nodes:\n        current_dist = distance_matrix[current_node][node]\n        dest_dist = distance_matrix[node][destination_node]\n\n        if remaining_nodes > 1:\n            connectivity_score = sum(1.0 / (1.0 + distance_matrix[node][n]) for n in unvisited_nodes if n != node) / (remaining_nodes - 1)\n            flow_disruption = max(distance_matrix[node][n] for n in unvisited_nodes if n != node) - current_dist\n            node_degree = sum(1 for n in range(total_nodes) if distance_matrix[node][n] > 0 and n != node)\n            centrality_score = node_degree / (total_nodes - 1)\n        else:\n            connectivity_score = 0\n            flow_disruption = 0\n            centrality_score = 0\n\n        # Novelty bonus based on historical visits\n        novelty_bonus = 0\n        if historical_path and node in historical_path:\n            novelty_bonus = -0.1 * (historical_path.count(node) / len(historical_path))\n\n        # Dynamic weight adjustments\n        weight_local = 0.4 * exploitation_weight + 0.2 * exploration_weight\n        weight_connectivity = 0.3 * exploitation_weight + 0.1 * exploration_weight\n        weight_dest = -0.2 * exploitation_weight + 0.1 * exploration_weight\n        weight_flow = -0.1 * flow_disruption if flow_disruption > 0 else 0\n        weight_centrality = 0.2 * exploitation_weight + 0.1 * exploration_weight\n        weight_novelty = 0.3 * exploration_weight\n\n        score = (weight_local * current_dist) + (weight_connectivity * connectivity_score) + \\\n                (weight_flow * flow_disruption) + (weight_dest * dest_dist) + \\\n                (weight_centrality * centrality_score) + (weight_novelty * novelty_bonus)\n\n        scores.append((node, score))\n\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
     "objective": 6.28604,
     "other_inf": null
}
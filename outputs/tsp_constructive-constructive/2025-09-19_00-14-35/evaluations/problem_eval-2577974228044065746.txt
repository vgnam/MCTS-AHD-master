import math
import random

def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    total_nodes = len(distance_matrix)
    remaining_nodes = len(unvisited_nodes)
    exploration_factor = remaining_nodes / total_nodes

    # Adaptive temperature for exploration control
    temperature = math.exp(-0.5 * exploration_factor)

    # Path entropy calculation for diversity
    path_entropy = -sum((count / total_nodes) * math.log(count / total_nodes) if count > 0 else 0
                        for count in [sum(1 for n in unvisited_nodes if n == node) for node in range(total_nodes)])

    scores = []
    for node in unvisited_nodes:
        current_dist = distance_matrix[current_node][node]
        dest_dist = distance_matrix[node][destination_node]

        # Connectivity and centrality metrics
        connectivity_score = sum(1.0 / (1.0 + distance_matrix[node][n]) for n in unvisited_nodes if n != node) / (remaining_nodes - 1) if remaining_nodes > 1 else 0
        node_degree = sum(1 for n in range(total_nodes) if distance_matrix[node][n] > 0 and n != node)
        centrality_score = node_degree / (total_nodes - 1)

        # Reinforcement learning-inspired scoring
        reward = -current_dist + 0.5 * connectivity_score - 0.3 * dest_dist + 0.2 * centrality_score
        exploration_bonus = temperature * random.uniform(-1, 1)
        entropy_bonus = 0.1 * path_entropy

        # Dynamic weight adjustments
        weight_reward = 0.7 - 0.3 * exploration_factor
        weight_exploration = 0.3 * exploration_factor
        weight_entropy = 0.2 * exploration_factor

        score = (weight_reward * reward) + (weight_exploration * exploration_bonus) + (weight_entropy * entropy_bonus)
        scores.append((node, score))

    # Temperature-based selection
    if random.random() < temperature:
        next_node = random.choice(unvisited_nodes)
    else:
        next_node = max(scores, key=lambda x: x[1])[0]

    return next_node

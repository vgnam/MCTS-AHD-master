def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    if not unvisited_nodes:
        return destination_node

    remaining_nodes = len(unvisited_nodes)
    total_nodes = len(distance_matrix)
    exploration_factor = remaining_nodes / total_nodes

    # Calculate node centrality as inverse of average distance to all other nodes
    centrality = {node: 1.0 / (sum(distance_matrix[node]) / (total_nodes - 1)) for node in unvisited_nodes}

    # Initialize historical traversal costs (simulated for this example)
    historical_costs = {node: 1.0 for node in unvisited_nodes}

    scores = []
    for node in unvisited_nodes:
        current_dist = distance_matrix[current_node][node]
        dest_dist = distance_matrix[node][destination_node]

        if remaining_nodes > 1:
            # Adaptive connectivity score with centrality weighting
            connectivity_score = sum(centrality[n] / (1.0 + distance_matrix[node][n]) for n in unvisited_nodes if n != node) / (remaining_nodes - 1)
            flow_disruption = max(distance_matrix[node][n] for n in unvisited_nodes if n != node) - current_dist
        else:
            connectivity_score = 0
            flow_disruption = 0

        # Dynamic weight scaling with exploration factor and centrality
        weight_local = 0.4 + 0.2 * exploration_factor
        weight_connectivity = 0.3 + 0.1 * centrality[node]
        weight_dest = -0.3 * (1 - exploration_factor)
        weight_flow = -0.2 * flow_disruption if flow_disruption > 0 else 0

        # Reinforcement learning-inspired adjustment with momentum
        momentum = 0.1 * historical_costs[node]
        penalty_factor = 1.0 + (dest_dist * (1 + momentum)) / (sum(distance_matrix[current_node][n] for n in unvisited_nodes) + 1e-6)

        # Dynamic penalty for high historical costs
        history_penalty = 1.0 + 0.5 * historical_costs[node]

        score = (weight_local * current_dist) + (weight_connectivity * connectivity_score) + (weight_dest * dest_dist) + (weight_flow * flow_disruption) * penalty_factor * history_penalty

        scores.append((node, score))

    # Select node with minimum score (lower is better)
    next_node = min(scores, key=lambda x: x[1])[0]

    # Update historical costs (simulated for this example)
    if scores:
        historical_costs[next_node] = historical_costs.get(next_node, 1.0) * 1.1  # Increase cost for next traversal

    return next_node
